{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pursuant-recall",
   "metadata": {},
   "source": [
    "# First week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-stage",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-afternoon",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "Для начала рассматриваем текст как последовательность слов - токенов.\\\n",
    "Для этого необходимо разбивать текст на токены. Что и называется процессом токенизации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-vertex",
   "metadata": {},
   "source": [
    "## 2. Token normalization\n",
    "\n",
    "1) Стемминг\\\n",
    "    Процесс избавления и замены суффиксов для получения корня слова, который называется стем\\\n",
    "    Используются эвристики для обрезания суффиксов.\\\n",
    "2) Лемматизация\\\n",
    "    Процесс словарного и/или морфологического анализ для приведения слова к \"словарному\" виду\\\n",
    "    Используются правила построения слов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-broad",
   "metadata": {},
   "source": [
    "### 2.1 Пример: (nltk.stem.PorterStemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-nigeria",
   "metadata": {},
   "source": [
    "Под капотом находится грамматика(та самая, которая из ТРЯПа), которая стемит слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "appointed-balloon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw -> stemmed\n",
      "feet -> feet\n",
      "wolves -> wolv\n",
      "cats -> cat\n",
      "talked -> talk\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "raw_words = [\"feet\", \"wolves\", \"cats\", \"talked\"]\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "print(\"raw -> stemmed\")\n",
    "\n",
    "for word in raw_words:\n",
    "    print(word + \" -> \" + stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-improvement",
   "metadata": {},
   "source": [
    "### 2.2 Пример: (WordNet lemmatizer)\n",
    "Используется база данных WordNet(WordNet — это лексическая база данных английского языка) для поиска лемм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tight-tender",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Иван\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw -> lemmatized\n",
      "feet -> foot\n",
      "wolves -> wolf\n",
      "cats -> cat\n",
      "talked -> talked\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "raw_words = [\"feet\", \"wolves\", \"cats\", \"talked\"]\n",
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "print(\"raw -> lemmatized\")\n",
    "\n",
    "for word in raw_words:\n",
    "    print(word + \" -> \" + stemmer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-weather",
   "metadata": {},
   "source": [
    "# Feature extraction from text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-bahrain",
   "metadata": {},
   "source": [
    "## 1. Bag of words(BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-documentation",
   "metadata": {},
   "source": [
    "Идея состоит в том, чтобы посчитать появление каждого токена в нашем тексте.\\\n",
    "Мотивация: например в задаче сентимент-анализа, таким образом, будут помечаться слова с повышенной эмоциональной окраской, который могут стать сильным признаком для модели.\\\n",
    "Для каждого токена у нас есть столбец, такая техника называется: text vectorization.\\\n",
    "Минусы такого подхода:\n",
    "- теряется порядок слов, поэтому и называется мешок слов\\\n",
    "- счетчики токенов не нормализованы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-tuesday",
   "metadata": {},
   "source": [
    "## 2. N-grams\n",
    "Для того, чтобы учитывать какой-то порядок слов, мы можем рассматривать, пары, тройки слов и т.д.\\\n",
    "Минусы: \n",
    "- много фичей \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-drove",
   "metadata": {},
   "source": [
    "Данную проблему, можно решить, избавившись от некоторых n-грамм, основываясь на частоте их появления в текстах нашего корпуса.\\\n",
    "Нужно оставить лишь среднечастотные, т.к. высокочастотные н-граммы появляются почти во всех документах и ничего не значат, не способствуют разделению классов. В то же время, низкочастотные специфичны могут негативно повлиять на качество переобучением."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-tyler",
   "metadata": {},
   "source": [
    "### 3. TF-IDF\n",
    "#### TF - Term frequency\n",
    "Частота появление токена или н-граммы t в документе d.\\\n",
    "tf(t,d) = $\\frac{f_{t,d}}{\\sum f_{t',d}}$, $\\forall t' \\in d$\\\n",
    "log-normalization: $1 + \\log(f_{t,d})$\n",
    "\n",
    "#### IDF - inverse document frequency\n",
    "- $N = |D|$ - количество документов в корпусе\n",
    "- $|\\{d \\in D: t\\in d\\}|$ - количество документо, в которых встречается токен t\n",
    "- idf(t,d) = $\\log \\frac{N}{|\\{d \\in D: t\\in d\\}|}$\n",
    "\n",
    "#### TF-IDF\n",
    "tfidf(t, d, D) = tf(t, d) $\\cdot$ idf(t, D)\n",
    "Высокое значение tf-idf достигается за счет высокой частотности терма в документе и низкого количества документов, в котором это слово встречается."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-target",
   "metadata": {},
   "source": [
    "### Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cross-uruguay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good movie</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good movie      like     movie       not\n",
       "0    0.707107  0.000000  0.707107  0.000000\n",
       "1    0.577350  0.000000  0.577350  0.577350\n",
       "2    0.000000  0.707107  0.000000  0.707107\n",
       "3    0.000000  1.000000  0.000000  0.000000\n",
       "4    0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "texts = [\n",
    "    \"good movie\", \"not a good movie\", \"did not like\", \n",
    "    \"i like it\", \"good one\"\n",
    "]\n",
    "# using default tokenizer in TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-child",
   "metadata": {},
   "source": [
    "# Linear models for sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-hollywood",
   "metadata": {},
   "source": [
    "Логистическая регрессия + Bag of words является хорошим бейзлайном для работы с текстами.\\\n",
    "Глубокое обучение не дает сумасшедших добавок к качеству."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-arlington",
   "metadata": {},
   "source": [
    "# Hashing trick for spam filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-chemical",
   "metadata": {},
   "source": [
    "Если для обучения требуется очень много данных, то на хранение может потребоваться большое количество ресурсов, а если данные нужно параллельно синхронизировать, то совсем все плохо.\\\n",
    "В таком случае у нас существует HashingVectorizer, с помощью которого можно ограничивать количество итоговых фичей $\\phi = \\text{hash(ngramm)} \\% 2^{b}$.\\\n",
    "В теории могут происходить колизии, которые будут ухудшать качество модели, но на практике, начиная с некоторого большого b, качество не отличается.\\\n",
    "Помимо этого существует следующая техника, которая учитывает персонализированные фичи для пользователя для спам-неспам классификации. А именно, добавить в модель фичи типа \\<user_name>_\\<n-gramm>.\\\n",
    "Данная техника дает существенное улучшение качества."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-median",
   "metadata": {},
   "source": [
    "# Neural Networks for words\n",
    "## 1. word2vec embeddings\n",
    "Вместо разряженных матриц с onehotencoding векторами, в подходе с нейронными сетями используется более плотные представления слов и текста. Одним из таких подходов является word2vec embedding. Предобученная модель заменяет слово на плотный вектор с какими-то значениями, основное требование, чтобы слова с более похожими контекстами будут иметь почти коллиниарные вектора.\\\n",
    "Представлять текст будем суммированием таких векторов.(также как и для onehotencoding подхода).\n",
    "## 2. 1D-Convolution\n",
    "Лучшим решением будет представлять текст матрицей из последовательных векторов-слов, а поверх этой матрицы использовать сверточные фильтры.\\\n",
    "Подход заключается в том, чтобы взять наш текст, построить word2vec embedding'и, составить матрицу из этих строк-векоторов, далее прогнать 3,4,5 - gram скользящие окна со 100 разными фильтрами, на каждый фильтр мы будем использовать max pooling, что будет нам возвращать максимальное значение активации на каждом фильтре, получим фиксированный output вектор, который уже можно будет отправлять в модель. Что дает прирост в качестве."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-skating",
   "metadata": {},
   "source": [
    "# Neural networks for charachters\n",
    "##  1. Текст как последовательность букв\n",
    "В отличие от предыдущих подходов, здесь мы рассмотрим текст как последовательность символов.\\\n",
    "Допустим у нас есть алфавит символов. Сделаем ohe для каждого символа и запустим сверточные фильтры поверх них и используем скользящее окно max pooling для двух соседних значений для каждого слоя, потом получим матрицу, поверх нее снова повторим процесс.\\\n",
    "По оценкам качества на больших датасетах, подходы глубокого обучения выигрывают у классических методов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
